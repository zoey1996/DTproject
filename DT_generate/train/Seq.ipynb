{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef25810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import argparse\n",
    "from utils import set_seed\n",
    "import numpy as np\n",
    "import wandb\n",
    "import math\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "from model import GPT, GPTConfig\n",
    "from trainer import Trainer, TrainerConfig\n",
    "\n",
    "from seq_embedd import SmilesDataset\n",
    "import selfies as sf\n",
    "from PyBioMed.PyProtein import CTD\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8253bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "run_name = \"Transport_seq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e0ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"DTproject\", name=run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd059898",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../datasets/chemb_drug_selfies.csv')\n",
    "data = data.dropna(axis=0).reset_index(drop=True)\n",
    "data.columns = data.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(axis=0).reset_index(drop=True)\n",
    "data.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4800c497",
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_seq = pd.read_csv(\"../datasets/transport_pro_seq.txt\",sep='\\t')\n",
    "pro_seq = pro_seq.dropna(axis=0).reset_index(drop=True)\n",
    "pro_seq = pro_seq.rename(columns={\"uniprot_id\":\"dt\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7ae063",
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_seq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff5eaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data = pd.merge(data,pro_seq,how=\"right\",on=\"dt\")\n",
    "merge_data = merge_data.dropna(axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dcf656",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13dd48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get selfies train and validation datasets\n",
    "\n",
    "train_data = merge_data[merge_data['split'] == 'train'].reset_index(drop=True)\n",
    "val_data = merge_data[merge_data['split'] == 'test'].reset_index(drop=True)\n",
    "\n",
    "selfies_list = list(train_data['selfies'])\n",
    "vselfies_list = list(val_data['selfies'])\n",
    "\n",
    "print(len(selfies_list))\n",
    "print(len(vselfies_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450df4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get All charsets from datasets\n",
    "\n",
    "from torchtext.legacy import data as d\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "\n",
    "all_selfies = data['selfies'].to_list()\n",
    "BLANK_WORD = \"<blank>\"\n",
    "tokenizer = lambda x: x.split()\n",
    "TGT = d.Field(tokenize=tokenizer,pad_token=BLANK_WORD)\n",
    "src = []\n",
    "src_len = []\n",
    "for i in all_selfies:\n",
    "    i = i[2:-2].replace(\"\\\\\\\\\",\"\\\\\")\n",
    "    src.append(i.split(\"', '\"))\n",
    "    src_len.append(len(i.split(\"', '\")))\n",
    "#max_len = max(src_len) + 2\n",
    "\n",
    "TGT.build_vocab(src)\n",
    "#vocab_size = len(TGT.vocab.freqs.most_common()) + 3\n",
    "\n",
    "\n",
    "whole_string = []\n",
    "for k in TGT.vocab.stoi.keys():\n",
    "    whole_string.append(k)\n",
    "print(len(whole_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597798ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get All charsets index\n",
    "stoi = json.load(open(f'../datasets/drug_selfies_stoi.json', 'r'))\n",
    "itos = dict(zip(stoi.values(), stoi.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1930dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the longest string to be flattened later\n",
    "\n",
    "max_len = max(src_len)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2cceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treat selfies as inputs of equal length to guarantee that the input model does not have dimensional problems\n",
    "\n",
    "selfies = []\n",
    "BLANK_WORD = '<blank>'\n",
    "for s in selfies_list:\n",
    "    s = eval(s)\n",
    "    while len(s) < max_len+1:   #In case the end information is lost\n",
    "        s.append(BLANK_WORD)\n",
    "    \n",
    "    selfies.append(s)\n",
    "    \n",
    "vselfies = [] \n",
    "#BOS_WORD = '<s>'\n",
    "#EOS_WORD = '</s>'\n",
    "BLANK_WORD = '<blank>'\n",
    "for vs in vselfies_list:\n",
    "    vs = eval(vs)\n",
    "    while len(vs) < max_len+1:  #In case the end information is lost\n",
    "        vs.append(BLANK_WORD)\n",
    "    \n",
    "    vselfies.append(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3ec718",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain protein sequence conditions\n",
    "\n",
    "pro = train_data[\"seq\"]\n",
    "vpro = val_data[\"seq\"]\n",
    "\n",
    "#Obtain protein sequence embedding length\n",
    "pro_len=147"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefcdd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SmilesDataset(selfies,whole_string,stoi,itos,max_len,aug_prob=0,pro=pro)\n",
    "valid_dataset = SmilesDataset(vselfies,whole_string,stoi,itos,max_len,aug_prob=0,pro=vpro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b91b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "n_layer = 8\n",
    "n_head = 8\n",
    "n_embd = 256\n",
    "\n",
    "max_epochs = 10\n",
    "batch_size = 16\n",
    "learning_rate = 6e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f975dfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.max_len, pro_len=pro_len,  # args.num_props,\n",
    "                        n_layer=n_layer, n_head=n_head, n_embd=n_embd,\n",
    "                        lstm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8821fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe63cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tconf = TrainerConfig(max_epochs=max_epochs, batch_size=batch_size, \n",
    "                      learning_rate=learning_rate,\n",
    "                      lr_decay=True, warmup_tokens=0.1*len(train_data)*max_len, \n",
    "                      final_tokens= max_epochs*len(train_data)*max_len,\n",
    "                      num_workers=0, \n",
    "                      ckpt_path=f'../result/models/{run_name}.pt', \n",
    "                      block_size=train_dataset.max_len, generate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d172d334",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, train_dataset, valid_dataset,\n",
    "                  tconf, train_dataset.stoi, train_dataset.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0023b6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trainer.train(wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f416d6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DTproject",
   "language": "python",
   "name": "dtproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
